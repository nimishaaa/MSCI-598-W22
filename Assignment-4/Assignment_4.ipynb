{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRyiR4JHXaDe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional,Flatten\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = \"./\""
      ],
      "metadata": {
        "id": "RlI-EEGtYy62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from files and save it into lists\n",
        "def data_load(f, data):\n",
        "    for line in f.readlines():\n",
        "        line_str = \"\"\n",
        "        line = line[1:-2].split(r\", \")\n",
        "        for i in range(len(line)):\n",
        "            line[i] = line[i][1:-1]\n",
        "            line_str += str(line[i]+\" \")\n",
        "        data.append(line_str)\n",
        "    \n",
        "data = []\n",
        "target = []\n",
        "\n",
        "for file_name in [\"val.csv\",\n",
        "          \"val_ns.csv\",\n",
        "          \"training.csv\",\n",
        "          \"training_ns.csv\",\n",
        "          \"test.csv\",\n",
        "          \"test_ns.csv\",\n",
        "          \"out.csv\",\n",
        "          \"out_ns.csv\"]:\n",
        "    file_open = open(data_directory + file_name,'r')\n",
        "    load_data(file_open,data)\n",
        "    file_open.close()"
      ],
      "metadata": {
        "id": "aa6PFf27Ylpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_seq = [text_to_word_sequence(sent) for sent in data]\n",
        "MAX_SENT_LEN = int(np.percentile([len(seq) for seq in word_seq], 90))\n",
        "print('90th Percentile Sentence Length:', MAX_SENT_LEN)\n",
        "\n",
        "# cut every sentence  according to MAX_SENT_LEN\n",
        "data = [' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq]\n",
        "\n",
        "# Convert the sequence of words to sequnce of indices\n",
        "MAX_VOCAB_SIZE = 80000\n",
        "tokenizer = Tokenizer(MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(data)\n",
        "data = tokenizer.texts_to_sequences(data)\n",
        "data = pad_sequences(data, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyFbNI6JY8Ng",
        "outputId": "0104f466-d232-4481-cb7b-0a23e2ae3625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90th Percentile Sentence Length: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(file_path):\n",
        "\ttraining['target'] = 1\n",
        "\ttraining_ns['target'] = 0\n",
        "\ttrain = pd.concat([training,training_ns])\n",
        "\ttrain = shuffle(train, random_state=0).reset_index(drop=True)\n",
        "\n",
        "\ttest['target'] = 1\n",
        "\ttest_ns['target'] = 0\n",
        "\ttest = pd.concat([test,test_ns])\n",
        "\ttest = shuffle(test, random_state=0).reset_index(drop=True)\n",
        "\n",
        "\tval['target'] = 1\n",
        "\tval_ns['target'] = 0\n",
        "\tvalidation = pd.concat([val,val_ns])\n",
        "\tvalidation = shuffle(validation, random_state=0).reset_index(drop=True)\n",
        "\n",
        "\tx_train = train['Review'].to_numpy()\n",
        "\ty_train = train['target'].to_numpy()\n",
        "\tx_test = test['Review'].to_numpy()\n",
        "\ty_test = test['target'].to_numpy()\n",
        "\tx_validation = validation['Review'].to_numpy()\n",
        "\ty_validation = validation['target'].to_numpy()\n",
        "\n",
        "\tdoc = pd.concat([train,test,validation])"
      ],
      "metadata": {
        "id": "oGsipjYor_hB"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}